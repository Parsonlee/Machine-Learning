* 权重衰减（WeightDecay）：
  * 通过添加惩罚项L2（L2正则，岭回归-正则项*元素平方和）使得模型参数不会过大，从而控制模型复杂度
  * 正则项权重是控制模型复杂度的超参数

* 丢弃法（Dropout）：
  * 将一些输出项随机置0来控制模型复杂度
  * 常作用在MLP的隐藏层输出上
  * 丢弃概率是控制模型复杂度的超参数
  
* 数值稳定性：
  * 合理的权重初始值和激活函数的选取可以提高数值稳定性

* 批量归一化：
  * 一般用于深层网络，浅层效果不好
  * 固定小批量中的均值和方差，然后学习出适合的偏移和缩放
  * 可以加快收敛，一般不改变模型精度

* 微调（fine tuning）：
  * 使用已经训练过了的模型，但要使用更强的正则化
  * 使用更小的学习率
  * 使用更少的数据迭代次数