* 权重衰减（WeightDecay）：
  * 通过添加惩罚项L2（L2正则，岭回归-正则项*元素平方和）使得模型参数不会过大，从而控制模型复杂度
  * 正则项权重是控制模型复杂度的超参数

* 丢弃法（Dropout）：
  * 将一些输出项随机置0来控制模型复杂度
  * 常作用在MLP的隐藏层输出上
  * 丢弃概率是控制模型复杂度的超参数
  
* 数值稳定性：
  * 合理的权重初始值和激活函数的选取可以提高数值稳定性

* 微调（fine tuning）：
  * 使用已经训练过了的模型，但要使用更强的正则化
  * 使用更小的学习率
  * 使用更少的数据迭代次数

* 批量归一化（BatchNorm）：
  * 一般用于深层网络，浅层效果不好
  * 固定小批量中的均值和方差，然后学习出适合的偏移和缩放
  * 可以加快收敛，一般不改变模型精度

* 层归一化（LayerNorm）：
  * 对每条样本的所有特征做归一化

* BN和LN的对比：  
  BatchNorm： 对一个batch-size样本内的每个特征做归一化
  LayerNorm： 针对每条样本，对每条样本的所有特征做归一化

  对于2D输入来说，BatchNorm就是对每列特征做归一化，LayerNorm就是对每行做归一化。

  差异：
  * 如果你的特征依赖不同样本的统计参数，那BatchNorm更有效， 因为它不考虑不同特征之间的大小关系，但是保留不同样本间的大小关系
  * NLP领域适合用LayerNorm， CV适合BatchNorm，
  * 对于NLP来说，它不考虑不同样本间的大小关系，保留样本内不同特征之间的大小关系