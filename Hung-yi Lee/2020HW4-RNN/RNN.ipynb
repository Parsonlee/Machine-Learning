{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JLTkcaCGqXI"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpEGGtFPGqXM"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NXMOdTfyGqXN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_training_data(path='training_label.txt'):\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[1:] for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='testing_data.txt'):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [''.join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]]\n",
        "        X = [sent.split(' ') for sent in lines]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    outputs[outputs >= 0.5] = 1\n",
        "    outputs[outputs < 0.5] = 0\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_XpSTM-GqXP"
      },
      "source": [
        "## Train Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nghhkNmXGqXQ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Loading training data...')\n",
        "    train_x, train_y = load_training_data('./data/training_label.txt')\n",
        "    trian_x_no_label = load_training_data('./data/training_nolabel.txt')\n",
        "\n",
        "    print('Loading testing data...')\n",
        "    test_x = load_testing_data('./data/testing_data.txt')\n",
        "\n",
        "    model = train_word2vec(train_x + test_x)\n",
        "\n",
        "    print('saving model...')\n",
        "    model.save('./w2v_all.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr_Sw9mTGqXR"
      },
      "source": [
        "## Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zHd_11ZIGqXR"
      },
      "outputs": [],
      "source": [
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path='./w2v.model'):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sen_len = sen_len\n",
        "        self.sentences = sentences\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "\n",
        "    def get_w2v_model(self):\n",
        "        self.embedding = word2vec.Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "\n",
        "    def add_embedding(self, word):\n",
        "        # 把word加进embedding，并赋予一个随机的vector\n",
        "        # word只会是<PAD>, <UNK>\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "\n",
        "    def make_embedding(self, load=True):\n",
        "        print('Get embedding ...')\n",
        "        if load:\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = i\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 将<PAD> <UNK>加进embedding里面\n",
        "        self.add_embedding('<PAD>')\n",
        "        self.add_embedding('<UNK>')\n",
        "        print('total words: {}'.format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "\n",
        "    def pad_sequence(self, sentence):\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx['<PAD>'])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "\n",
        "    def sentence_word2idx(self):\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx['<UNK>'])\n",
        "\n",
        "            # 将每个句子变成一个长度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "\n",
        "    def labels_to_tensor(self, y):\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0MtSJgGqXT"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QwFwrb6NGqXU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, x, y):\n",
        "        self.data = x\n",
        "        self.label = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None:\n",
        "            return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8qKMOAJGqXU"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6AEuWsvrGqXV"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM, self).__init__()\n",
        "        # 製作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state\n",
        "        x = x[:, -1, :]  # (batch_size, hidden_size)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLnCKC78GqXV"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZjsTsbspGqXW"
      },
      "outputs": [],
      "source": [
        "def training(batch_size, epochs, lr, model_dir, train, valid, model, device='cuda'):\n",
        "    # 每层的参数总数之和\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    # 需要更新的参数数目\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train()\n",
        "    loss = nn.BCELoss()     # binary cross entropy loss\n",
        "    t_batch = len(train)\n",
        "    v_batch = len(valid)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            labels = labels.to(device, dtype=torch.float)   # Loss fn only takes float as input\n",
        "\n",
        "            # print(f'label shape: {labels.shape}')\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            # print(f'after label shape: {labels.shape}')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            outputs.squeeze()   # 去掉最外面的 dimension，变成 (batch_size, 1)\n",
        "\n",
        "            loss_ = loss(outputs, labels)\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "            correct = evaluation(outputs, labels)\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss_.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "                epoch+1, i+1, t_batch, loss_.item(), correct/batch_size), end='\\r')\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.5f}'.format(total_loss/t_batch, total_acc/t_batch))\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                labels = labels.to(device, dtype=torch.float)\n",
        "\n",
        "                labels = labels.unsqueeze(-1)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                outputs.squeeze()\n",
        "                loss_ = loss(outputs, labels)\n",
        "                correct = evaluation(outputs, labels)\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss_.item()\n",
        "\n",
        "            print('Valid | Loss:{:.5f} Acc: {:.5f}'.format(total_loss/v_batch, total_acc/v_batch))\n",
        "            if total_acc > best_acc:\n",
        "                best_acc = total_acc\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with best acc: {:.3f}'.format(best_acc))\n",
        "        print('-'*50)\n",
        "        model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeGAUo5CGqXX"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "path_prefix = './'\n",
        "train_with_label = os.path.join(path_prefix, 'data/training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'data/training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'data/testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model')\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 20\n",
        "fix_embedding = True\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "lr = 0.001\n",
        "model_dir = path_prefix\n",
        "\n",
        "print('loading data...')\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# 對 input 跟 labels 做預處理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device)\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
        "\n",
        "train_dataset = TwitterDataset(X_train, y_train)\n",
        "val_dataset = TwitterDataset(X_val, y_val)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "training(batch_size, epochs, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "metadata": {
        "id": "zaT4u0mdGqXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ss4rb47GqXW"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Q3R6DfGqXW"
      },
      "outputs": [],
      "source": [
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs.squeeze()\n",
        "            outputs[outputs >= 0.5] = 1\n",
        "            outputs[outputs < 0.5] = 0\n",
        "\n",
        "            out = outputs.int().tolist()\n",
        "            out = [l[0] for l in out]\n",
        "            ret_output.extend(out)\n",
        "            \n",
        "\n",
        "    return ret_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLcadi2ZGqXX"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIEtRtV4GqXX",
        "outputId": "8fb25a15-175b-4560-ce3e-d6e8efbecef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading test data...\n",
            "Get embedding ...\n",
            "get words #974"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get words #24694\n",
            "total words: 24696\n",
            "sentence count #200000\n",
            "load model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save csv ...\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "print('loading test data...')\n",
        "test_x = load_testing_data(testing_data)\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(test_x, None)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# outputs\n",
        "\n",
        "tmp = pd.DataFrame({'id':[i for i in range(len(test_x))], 'label':outputs})\n",
        "print('save csv ...')\n",
        "tmp.to_csv(os.path.join(model_dir, 'submission.csv'), index=False)\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit ML2020spring-hw4 -f ./submission.csv -m LSTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAXrX0bqqD9T",
        "outputId": "7d3bde3f-968d-42d0-fe40-c0aef5c5987a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1.61M/1.61M [00:02<00:00, 571kB/s]\n",
            "Successfully submitted to ML2020spring - hw4"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0e963d61ae5592af78c1a29f643d857bd502442649449ec5cf61b348680e7239"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('dl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}